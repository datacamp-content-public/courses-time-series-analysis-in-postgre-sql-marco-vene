---
title: Insert title here
key: 45ab724914f22ea84c7d8d8737181126

---
## Aggregate Functions

```yaml
type: "TitleSlide"
key: "60ab9e22b3"
```

`@lower_third`

name: Marco Venè
title: Senior Data Analyst


`@script`
In the last exercise, you learnt the ```filter``` and  ```group by``` operations that allowed you to compute the sum of tickets sold by week in the months of January and February. 
In this lesson, you will expand your knowledge set and learn the most relevant aggregate functions for analyzing time series. 
With these new functions, you will be able to compute quickly your summary statistics and get additional insights from your data. Let’s dig into it!


---
## Code from last exercise

```yaml
type: "TwoColumns"
key: "60ddf54fc4"
center_content: false
```

`@part1`
Weekly Ticket Sales
```sql
SELECT
EXTRACT(year FROM timestamp_checkout) as year,
EXTRACT(week FROM timestamp_checkout) as week,
SUM(tickets) as tickets_sum
FROM sales
WHERE timestamp_checkout BETWEEN '2018-01-01' AND '2018-02-28'
GROUP BY 1,2
ORDER BY 1,2
;
```


`@part2`
| year | week | tickets_sum |
| ---- | ---- | ----------- |
| 2018 | 1    | 29          |
| 2018 | 2    | 25          |
| 2018 | 3    | 30          |
| 2018 | 4    | 25          |
| 2018 | 5    | 57          |
| 2018 | 6    | 22          |
| 2018 | 7    | 18          |
| 2018 | 8    | 27          |
| 2018 | 9    | 4           |


`@script`
After receiving the last report, the CEO asked you to uncover more insights on the customer transactions. 
He now wants to understand the if the number of ticket sold per transaction, also called basket size, is increasing over time. Aggregate functions can help you with your project.


---
## Basic aggregate functions

```yaml
type: "TwoColumns"
key: "70c450faef"
disable_transition: false
```

`@part1`
- `AVG()`: arithmetic mean {{1}}
- `MIN()`: minimum {{1}}
- `MAX()`: maximum {{1}}
- `COUNT(*)`: number of rows {{1}}
- `COUNT()`: number of rows with `not null` values {{1}}


`@part2`
Weekly Transactions Report{{2}}

```sql
SELECT
EXTRACT(year FROM timestamp_checkout) as year,
EXTRACT(week FROM timestamp_checkout) as week,
sum(tickets) as tickets_sum,
AVG(tickets) as basket_avg,
MIN(tickets) as basket_min,
MAX(tickets) as basket_max,
COUNT(*) as transactions_count,
COUNT(tickets) as transactions_count_not_null
FROM sales
WHERE timestamp_checkout BETWEEN '2018-01-01' AND '2018-02-28'
GROUP BY 1,2
ORDER BY 1,2
;
``` {{2}}


`@script`
PostgreSQL offers a handy set of functions to compute key summary statistics for a distribution. Here you can see some relevant functions to compute the mean, the most extreme data points, and the count of values in each set. 

With these, we can compute additional stats to our weekly sales report and get insights on trend of number of transactions, basket size and potential outliers.


---
## Cleaning output - the ROUND() function

```yaml
type: "TwoColumns"
key: "ba524527f4"
```

`@part1`
Weekly Average Basket Size

```sql
SELECT
EXTRACT(week FROM timestamp_checkout) as week,
AVG(tickets) as basket_avg,
ROUND(AVG(tickets)) as round_0,
ROUND(AVG(tickets),1) as round_1
FROM sales
WHERE timestamp_checkout BETWEEN '2018-01-01' AND '2018-02-28'
GROUP BY 1
ORDER BY 1
```


`@part2`
| week | basket_avg        | round_0 | round_1 |
| ---- | ------------------ | ------- | ------- |
| 1    | 4.1428571428571429 | 4       | 4.1     |
| 2    | 3.5714285714285714 | 4       | 3.6     |
| 3    | 3.3333333333333333 | 3       | 3.3     |
| 4    | 5.0000000000000000 | 5       | 5.0     |
| 5    | 6.3333333333333333 | 6       | 6.3     |
| 6    | 3.6666666666666667 | 4       | 3.7     |
| 7    | 2.2500000000000000 | 2       | 2.3     |
| 8    | 5.4000000000000000 | 5       | 5.4     |
| 9    | 1.3333333333333333 | 1       | 1.3     |


`@script`
When computing aggregate functions, by default PostgreSQL outputs numeric values with many decimals. This is in most of the cases not the optimal display of your data.
With the round function you can round numeric values with as many decimals as you prefer and get a cleaner report. The round function take as first argument a numeric value and as second argument the number of decimal places you want.


---
## Other rounding methods relevant for time series

```yaml
type: "TwoColumns"
key: "4f238e66d5"
```

`@part1`
- `FLOOR()`: round to nearest lower integer {{1}}
- `CEILING()`: round to nearest greater integer{{1}}

```sql
SELECT
EXTRACT(week FROM timestamp_checkout) as week,
SUM(tickets) as tickets,
FLOOR(SUM(tickets)/10.0)*10 as floor_10,
CEILING(SUM(tickets)/10.0)*10 as ceil_10
FROM sales
WHERE timestamp_checkout BETWEEN '2018-01-01' AND '2018-02-28'
GROUP BY 1
ORDER BY 1
```{{2}}


`@part2`
| week | tickets | floor_10 | ceil_10 |
| ---- | ------- | -------- | ------- |
| 1    | 29      | 20       | 30      |
| 2    | 25      | 20       | 30      |
| 3    | 30      | 30       | 30      |
| 4    | 25      | 20       | 30      |
| 5    | 57      | 50       | 60      |
| 6    | 22      | 20       | 30      |
| 7    | 18      | 10       | 20      |
| 8    | 27      | 20       | 30      |
| 9    | 4       | 0        | 10      |
{{2}}


`@script`
When it comes to time series, it is often relevant to apply other rounding methods to your data. For instance, you may want to round your values to the nearest lower or greater integer or to a defined multiple. In our example, let’s say that the finance department wants a report of the weekly ticket sales total, but they are not interested in the exact values, but require the rounded values at a multiple of 10 instead.  

The floor and ceiling functions, along with some basic arithmetic computation can help building this report. They take as argument a numeric value and return the closest lowest and greatest integers respectively.

In our example, first we divide the totals by 10.0 to get a decimal value to round, then we apply the floor and ceiling function, and finally we multiply by 10 again to get the rounded ticket totals to the nearest lower and greater multiple of 10.

With this simple workflow you can round values at any multiple and customize your time series reports.


---
## Aggregate functions for statistics

```yaml
type: "TwoColumns"
key: "c455f131e6"
```

`@part1`
- `VAR_POP()`: variance {{1}}
- `STDDEV_POP()`: standard deviation {{1}}

```sql
SELECT
EXTRACT(month FROM timestamp_checkout) as month,
ROUND(AVG(tickets),2) as basket_avg,
ROUND(VAR_POP(tickets),2) as basket_var,
ROUND(STDDEV_POP(tickets),2) as basket_sd,
MAX(tickets) - MIN(tickets) as basket_range
FROM sales
WHERE timestamp_checkout BETWEEN '2018-01-01' AND '2018-02-28'
GROUP BY 1
ORDER BY 1
``` {{2}}


`@part2`
Monthly basket size disperison{{2}}


| month | basket_avg | basket_var | basket_sd | basket_range |
| ----- | ----------- | ----------- | ---------- | ------------- |
| 1     | 4.35        | 15.84       | 3.98       | 18            |
| 2     | 3.64        | 8.80        | 2.97       | 14            |
{{2}}


`@script`
Now that you know how to compute the basic aggregate functions and round the output for a cleaner and more customized report, it is time to explore another set of summary functions for statistical analysis. 

The var_pop and stddev_pop functions compute the variance and standard deviation respectively. “Pop” stands for population and distinguishes this set of functions from var_samp and stddev_samp, which compute the variance and standard deviation for the sample instead. 

Along with the range, which can be computed using max and min functions, the variance and the standard deviation are the most important metrics to quantify the dispersion around the mean of a variable.

In our example, we compute the monthly average ticket sold per transaction, together with the data dispersion metrics. With this, we can understand the distribution of basket sizes in January and February.

The CEO is worried that the basket size is lower in February.


---
## Let's aggregate data!

```yaml
type: "FinalSlide"
key: "ed71688247"
```

`@script`
Now it’s your turn to aggregate some time series data, let’s try!

